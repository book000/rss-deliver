import { BaseService } from '@/base-service'
import { Logger } from '@book000/node-utils'
import CollectResult, { Item } from '@/model/collect-result'
import ServiceInformation from '@/model/service-information'
import axios from 'axios'
import * as cheerio from 'cheerio'
import fs from 'node:fs'
import crypto from 'node:crypto'
import sharp from 'sharp'

/**
 * contentsInfo API ã®ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
 */
interface PageContentInfo {
  /** ç”»åƒURLï¼ˆç½²åä»˜ãï¼‰ */
  imageUrl: string
  /** ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«é…åˆ—ï¼ˆJSONæ–‡å­—åˆ—ï¼‰ */
  scramble: string
  /** ã‚½ãƒ¼ãƒˆé † */
  sort: number
  /** ç”»åƒã®å¹… */
  width: number
  /** ç”»åƒã®é«˜ã• */
  height: number
  /** æœ‰åŠ¹æœŸé™ */
  expiresOn: number
}

/**
 * contentsInfo API ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
 */
interface ContentsInfoResponse {
  /** ç·ãƒšãƒ¼ã‚¸æ•° */
  totalPages: number
  /** ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ–¹å‘ */
  scrollDirection: string
  /** è¦‹é–‹ãæŒ‡å®š */
  spreadDesignation: number
  /** ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒšãƒ¼ã‚¸é †ã«æ ¼ç´ã•ã‚ŒãŸé…åˆ—ï¼‰ */
  result: PageContentInfo[]
}

/**
 * ãƒãƒ—ãƒ†ãƒ”ãƒ”ãƒƒã‚¯ RSS ã‚µãƒ¼ãƒ“ã‚¹
 *
 * ç«¹ã‚³ãƒŸï¼ã‹ã‚‰ãƒãƒ—ãƒ†ãƒ”ãƒ”ãƒƒã‚¯ã®æ¼«ç”»æƒ…å ±ã‚’åé›†ã—ã€RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹
 */
export default class PopTeamEpic extends BaseService {
  private title: string | null = null
  private link: string | null = null
  private image: string | null = null
  private siteName: string | null = null

  information(): ServiceInformation {
    if (!this.title || !this.link || !this.image || !this.siteName) {
      throw new Error('Service is not initialized')
    }
    return {
      title: this.title,
      link: this.link,
      description: `${this.title} / å¤§å·ã¶ãã¶ / ${this.siteName}`,

      image: {
        url: this.image,
        title: this.title,
        link: this.link,
      },
      generator: 'book000/rss-deliver',
      language: 'ja',
    }
  }

  async collect(): Promise<CollectResult> {
    const logger = Logger.configure('PopTeamEpic::collect')

    const activeSeason = await this.fetchActiveSeason()
    if (!activeSeason) {
      logger.warn('â— No active season found')
      return {
        status: false,
        items: [],
      }
    }

    logger.info(`ğŸ‘€ ${activeSeason.title} ${activeSeason.url}`)

    this.title = activeSeason.title
    this.link = activeSeason.url
    this.image = activeSeason.image
    this.siteName = activeSeason.siteName

    const items = await this.collectTakecomicItems(activeSeason.url)

    return {
      status: true,
      items,
    }
  }

  async fetchActiveSeason(): Promise<{
    title: string
    url: string
    image: string
    siteName: string
    source: 'takecomic'
  } | null> {
    return this.fetchTakecomicSeason()
  }

  private async fetchTakecomicSeason(): Promise<{
    title: string
    url: string
    image: string
    siteName: string
    source: 'takecomic'
  } | null> {
    const logger = Logger.configure('PopTeamEpic::fetchTakecomicSeason')
    const takecomicSeriesUrl = 'https://takecomic.jp/series/8f3616ce97c36'
    const response = await axios.get(takecomicSeriesUrl, {
      validateStatus: () => true,
      headers: {
        'User-Agent':
          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        Accept:
          'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
      },
    })
    if (response.status !== 200) {
      logger.warn(
        `â— Failed to fetch takecomic series page (status=${response.status})`
      )
      return null
    }

    const $ = cheerio.load(response.data)
    const title = $('meta[property="og:title"]').attr('content')?.trim() ?? ''
    const image = PopTeamEpic.normalizeUrl(
      $('meta[property="og:image"]').attr('content') ?? ''
    )
    if (!title || !image) {
      return null
    }
    return {
      title,
      url: takecomicSeriesUrl,
      image,
      siteName: 'ç«¹ã‚³ãƒŸï¼',
      source: 'takecomic',
    }
  }

  /**
   * ã‚·ãƒªãƒ¼ã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’åé›†ã™ã‚‹
   *
   * @param seriesUrl ã‚·ãƒªãƒ¼ã‚ºãƒšãƒ¼ã‚¸ã® URL
   * @returns RSS ã‚¢ã‚¤ãƒ†ãƒ ã®é…åˆ—
   */
  private async collectTakecomicItems(seriesUrl: string): Promise<Item[]> {
    const logger = Logger.configure('PopTeamEpic::collectTakecomicItems')
    const response = await axios.get(seriesUrl, {
      validateStatus: () => true,
      headers: {
        'User-Agent':
          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        Accept:
          'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
      },
    })
    if (response.status !== 200) {
      throw new Error(`Failed to fetch: ${response.status}`)
    }
    const $ = cheerio.load(response.data)

    const items: Item[] = []
    const episodes = this.extractTakecomicEpisodes($)
    for (const episode of episodes) {
      const date = this.parseJstDate(episode.date)

      // ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ç”»åƒã‚’å–å¾—
      const imageUrls = await this.fetchEpisodeImages(episode.url, logger)

      // ç”»åƒãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯ã‚µãƒ ãƒã‚¤ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      if (imageUrls.length === 0 && episode.thumbnailUrl) {
        logger.warn(`âš ï¸ Falling back to thumbnail for ${episode.title}`)
        const thumbnailUrls = await this.saveImages(
          [episode.thumbnailUrl],
          logger
        )
        if (thumbnailUrls.length > 0) {
          imageUrls.push(...thumbnailUrls)
        }
      }

      if (imageUrls.length === 0) {
        logger.warn(`â— No images for ${episode.title}`)
        continue
      }

      const itemTitle = episode.date
        ? `${episode.date} ${episode.title}`
        : episode.title
      logger.info(`ğŸ“ƒ ${itemTitle} ${episode.url} (${imageUrls.length} images)`)
      items.push({
        title: itemTitle,
        link: episode.url,
        'content:encoded': imageUrls
          .map((url) => `<img src="${url}">`)
          .join('<br>'),
        ...(date ? { pubDate: date.toUTCString() } : {}),
      })
    }
    return items
  }

  /**
   * ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ç”»åƒã‚’å–å¾—ã™ã‚‹
   *
   * @param episodeUrl ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã® URL
   * @param logger ãƒ­ã‚¬ãƒ¼
   * @returns ç”»åƒ URL ã®é…åˆ—
   */
  private async fetchEpisodeImages(
    episodeUrl: string,
    logger: Logger
  ): Promise<string[]> {
    try {
      // ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‚’å–å¾—
      const response = await axios.get(episodeUrl, {
        validateStatus: () => true,
        headers: {
          'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
          Accept:
            'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
          'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        },
      })
      if (response.status !== 200) {
        logger.warn(`â— Failed to fetch episode page (${response.status})`)
        return []
      }

      // viewerId ã‚’æŠ½å‡º
      const viewerId = this.extractViewerId(response.data)
      if (!viewerId) {
        logger.warn('â— viewerId not found in episode page')
        return []
      }

      // contentsInfo API ã‚’å‘¼ã³å‡ºã—
      const contentsInfo = await this.fetchContentsInfo(viewerId, episodeUrl)
      if (!contentsInfo || contentsInfo.totalPages === 0) {
        logger.warn('â— No pages found in contentsInfo')
        return []
      }

      logger.info(`ğŸ“– Found ${contentsInfo.totalPages} pages`)

      // å„ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’å–å¾—ãƒ»å¾©å…ƒ
      const imageUrls: string[] = []
      for (let i = 0; i < contentsInfo.totalPages; i++) {
        const pageData = contentsInfo.result[i]
        if (!pageData) {
          continue
        }

        const imageUrl = await this.fetchAndUnscrambleImage(
          pageData,
          episodeUrl,
          logger
        )
        if (imageUrl) {
          imageUrls.push(imageUrl)
        }
      }

      return imageUrls
    } catch (error) {
      logger.warn(`â— Failed to fetch episode images: ${String(error)}`)
      return []
    }
  }

  /**
   * HTML ã‹ã‚‰ viewerId ã‚’æŠ½å‡ºã™ã‚‹
   *
   * @param html HTML æ–‡å­—åˆ—
   * @returns viewerId ã¾ãŸã¯ null
   */
  private extractViewerId(html: string): string | null {
    // data-comici-viewer-id å±æ€§ã‹ã‚‰ viewerId ã‚’æ¢ã™
    const dataAttrRegex = /data-comici-viewer-id="([a-f0-9]{32})"/i
    const dataAttrMatch = dataAttrRegex.exec(html)
    if (dataAttrMatch?.[1]) {
      return dataAttrMatch[1]
    }

    // __next_f (React Server Components) ã‹ã‚‰ viewerId ã‚’æ¢ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    const jsonRegex = /"viewerId":"([a-f0-9]{32})"/i
    const jsonMatch = jsonRegex.exec(html)
    return jsonMatch?.[1] ?? null
  }

  /**
   * contentsInfo API ã‚’å‘¼ã³å‡ºã™
   *
   * API ã¯ page-to ãŒå®Ÿéš›ã®ãƒšãƒ¼ã‚¸æ•°ã‚’è¶…ãˆã‚‹ã¨ 400 ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ãŸã‚ã€
   * ã¾ãš page-to=1 ã§ totalPages ã‚’å–å¾—ã—ã€ãã®å¾Œå…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹
   *
   * @param viewerId ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ ID
   * @param episodeUrl ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ URLï¼ˆReferer ãƒ˜ãƒƒãƒ€ãƒ¼ã«ä½¿ç”¨ï¼‰
   * @returns API ãƒ¬ã‚¹ãƒãƒ³ã‚¹
   */
  private async fetchContentsInfo(
    viewerId: string,
    episodeUrl: string
  ): Promise<ContentsInfoResponse | null> {
    const logger = Logger.configure('PopTeamEpic::fetchContentsInfo')

    const headers = {
      Origin: 'https://takecomic.jp',
      Referer: episodeUrl,
      'User-Agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
      Accept: 'application/json, text/plain, */*',
      'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
    }

    // ã‚¹ãƒ†ãƒƒãƒ—1: page-to=1 ã§ totalPages ã‚’å–å¾—
    const initialParams = new URLSearchParams({
      'user-id': '',
      'comici-viewer-id': viewerId,
      'page-from': '0',
      'page-to': '1',
    })
    const initialUrl = `https://takecomic.jp/api/book/contentsInfo?${initialParams.toString()}`

    logger.info(`ğŸ“¡ Fetching totalPages: viewerId=${viewerId}`)

    const initialResponse = await axios.get<ContentsInfoResponse>(initialUrl, {
      validateStatus: () => true,
      headers,
    })

    if (initialResponse.status !== 200) {
      logger.warn(
        `âŒ API error (initial): status=${initialResponse.status}, viewerId=${viewerId}`
      )
      return null
    }

    const totalPages = initialResponse.data.totalPages
    logger.info(`ğŸ“– totalPages=${totalPages}`)

    if (totalPages <= 1) {
      // 1 ãƒšãƒ¼ã‚¸ã®ã¿ã®å ´åˆã¯åˆå›ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãã®ã¾ã¾è¿”ã™
      return initialResponse.data
    }

    // ã‚¹ãƒ†ãƒƒãƒ—2: å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    const fullParams = new URLSearchParams({
      'user-id': '',
      'comici-viewer-id': viewerId,
      'page-from': '0',
      'page-to': String(totalPages),
    })
    const fullUrl = `https://takecomic.jp/api/book/contentsInfo?${fullParams.toString()}`

    const fullResponse = await axios.get<ContentsInfoResponse>(fullUrl, {
      validateStatus: () => true,
      headers,
    })

    if (fullResponse.status !== 200) {
      logger.warn(
        `âŒ API error (full): status=${fullResponse.status}, viewerId=${viewerId}`
      )
      return null
    }

    logger.info(`âœ… Fetched ${totalPages} pages successfully`)

    return fullResponse.data
  }

  /**
   * ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«ã•ã‚ŒãŸç”»åƒã‚’å–å¾—ã—ã€å¾©å…ƒã—ã¦ä¿å­˜ã™ã‚‹
   *
   * @param pageData ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
   * @param episodeUrl ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ URLï¼ˆReferer ãƒ˜ãƒƒãƒ€ãƒ¼ã«ä½¿ç”¨ï¼‰
   * @param logger ãƒ­ã‚¬ãƒ¼
   * @returns ä¿å­˜ã—ãŸç”»åƒã® URL
   */
  private async fetchAndUnscrambleImage(
    pageData: PageContentInfo,
    episodeUrl: string,
    logger: Logger
  ): Promise<string | null> {
    try {
      // ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCloudFront ç½²åä»˜ã URL ã«ã¯é©åˆ‡ãªãƒ˜ãƒƒãƒ€ãƒ¼ãŒå¿…è¦ï¼‰
      const response = await axios.get(pageData.imageUrl, {
        responseType: 'arraybuffer',
        validateStatus: () => true,
        headers: {
          Referer: episodeUrl,
          'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
          Accept:
            'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
        },
      })

      if (response.status !== 200) {
        logger.warn(`â— Failed to download image (${response.status})`)
        return null
      }

      const scrambledBuffer = Buffer.from(response.data)

      // ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«é…åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
      let scramble: number[]
      try {
        scramble = JSON.parse(pageData.scramble)
      } catch {
        logger.warn('â— Invalid scramble JSON, using original image')
        return await this.saveUnscrambledImage(scrambledBuffer)
      }

      // ç”»åƒã‚’å¾©å…ƒ
      const unscrambledBuffer = await this.unscrambleImage(
        scrambledBuffer,
        scramble
      )

      // ç”»åƒã‚’ä¿å­˜
      return await this.saveUnscrambledImage(unscrambledBuffer)
    } catch (error) {
      logger.warn(`â— Failed to unscramble image: ${String(error)}`)
      return null
    }
  }

  /**
   * ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«ã•ã‚ŒãŸç”»åƒã‚’å¾©å…ƒã™ã‚‹
   *
   * @param buffer ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«ã•ã‚ŒãŸç”»åƒãƒãƒƒãƒ•ã‚¡
   * @param scramble ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«é…åˆ—
   * @returns å¾©å…ƒã•ã‚ŒãŸç”»åƒãƒãƒƒãƒ•ã‚¡
   */
  private async unscrambleImage(
    buffer: Buffer,
    scramble: number[]
  ): Promise<Buffer> {
    // ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ï¼ˆé€šå¸¸ã¯ 4x4 = 16 ã‚¿ã‚¤ãƒ«ï¼‰
    const gridSize = Math.sqrt(scramble.length)
    if (!Number.isInteger(gridSize)) {
      // ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«ãªã—ã¨ã—ã¦å…ƒã®ç”»åƒã‚’è¿”ã™
      return buffer
    }

    // å…ƒç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰
    const image = sharp(buffer)
    const metadata = await image.metadata()

    // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã¯å…ƒã®ç”»åƒã‚’è¿”ã™
    if (!metadata.width || !metadata.height) {
      return buffer
    }

    // å®Ÿéš›ã®ç”»åƒã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
    const actualTileWidth = Math.floor(metadata.width / gridSize)
    const actualTileHeight = Math.floor(metadata.height / gridSize)

    // å„ã‚¿ã‚¤ãƒ«ã‚’æŠ½å‡ºï¼ˆclone() ã‚’ä½¿ç”¨ã—ã¦å†ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’é¿ã‘ã‚‹ï¼‰
    // Column-Major é †åºã§æŠ½å‡ºã™ã‚‹: (0,0), (0,1), (0,2), (0,3), (1,0)...
    const tiles: Buffer[] = []
    for (let c = 0; c < gridSize; c++) {
      for (let r = 0; r < gridSize; r++) {
        const left = c * actualTileWidth
        const top = r * actualTileHeight

        const tile = await image
          .clone()
          .extract({
            left,
            top,
            width: actualTileWidth,
            height: actualTileHeight,
          })
          .toBuffer()

        tiles.push(tile)
      }
    }

    // ã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«é…åˆ—ã«å¾“ã£ã¦ã‚¿ã‚¤ãƒ«ã‚’å†é…ç½®
    // scramble[i] = srcIndex
    // å‡ºåŠ›ã‚‚ Column-Major é †åºã§é…ç½®ã™ã‚‹
    const compositeOperations: sharp.OverlayOptions[] = []
    let f = 0
    for (let c = 0; c < gridSize; c++) {
      for (let r = 0; r < gridSize; r++) {
        const srcIndex = scramble[f]

        if (srcIndex >= 0 && srcIndex < tiles.length) {
          compositeOperations.push({
            input: tiles[srcIndex],
            left: c * actualTileWidth,
            top: r * actualTileHeight,
          })
        }
        f++
      }
    }

    // å¾©å…ƒç”»åƒã‚’ä½œæˆ
    const unscrambled = await sharp({
      create: {
        width: actualTileWidth * gridSize,
        height: actualTileHeight * gridSize,
        channels: 3,
        background: { r: 255, g: 255, b: 255 },
      },
    })
      .composite(compositeOperations)
      .jpeg({ quality: 90 })
      .toBuffer()

    return unscrambled
  }

  /**
   * å¾©å…ƒã—ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹
   *
   * @param buffer ç”»åƒãƒãƒƒãƒ•ã‚¡
   * @returns ä¿å­˜ã—ãŸç”»åƒã® URL
   */
  private async saveUnscrambledImage(buffer: Buffer): Promise<string> {
    if (!fs.existsSync('output/popute/')) {
      fs.mkdirSync('output/popute/', { recursive: true })
    }

    // ãƒˆãƒªãƒŸãƒ³ã‚°ã¨ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ 
    const processedBuffer = await sharp(buffer)
      .trim()
      .extend({
        top: 10,
        bottom: 10,
        left: 10,
        right: 10,
        background: { r: 255, g: 255, b: 255, alpha: 1 },
      })
      .jpeg({ quality: 90 })
      .toBuffer()

    const hash = this.hash(processedBuffer)
    fs.writeFileSync(`output/popute/${hash}.jpg`, processedBuffer)
    return `https://book000.github.io/rss-deliver/popute/${hash}.jpg`
  }

  /**
   * ã‚·ãƒªãƒ¼ã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
   *
   * @param $ cheerio ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
   * @returns ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æƒ…å ±ã®é…åˆ—ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€URLã€æ—¥ä»˜ã€ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒURLï¼‰
   */
  private extractTakecomicEpisodes($: ReturnType<typeof cheerio.load>): {
    title: string
    url: string
    date: string
    thumbnailUrl: string
  }[] {
    const episodes: {
      title: string
      url: string
      date: string
      thumbnailUrl: string
    }[] = []
    const seen = new Set<string>()

    for (const element of $('.series-eplist-item')) {
      const anchor = $(element).find('a.series-eplist-item-link').first()
      const href = anchor.attr('href') ?? ''
      if (!href) {
        continue
      }
      const url = PopTeamEpic.normalizeUrl(href)
      if (seen.has(url)) {
        continue
      }
      seen.add(url)
      const title = $(element).find('.series-eplist-item-h-text').text().trim()
      const date = $(element)
        .find('.series-eplist-item-meta-date')
        .text()
        .trim()
      // ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒURLã‚’å–å¾—ï¼ˆ-sm ã‚’ -lg ã«å¤‰æ›ï¼‰
      const thumbnailSrc = $(element).find('img').first().attr('src') ?? ''
      const thumbnailUrl = thumbnailSrc
        ? PopTeamEpic.normalizeUrl(
            thumbnailSrc.replace(/-sm\.(webp|png|jpe?g)$/i, '-lg.$1')
          )
        : ''
      episodes.push({ title, url, date, thumbnailUrl })
    }
    return episodes
  }

  private async saveImages(
    images: string[],
    logger: Logger
  ): Promise<string[]> {
    if (!fs.existsSync('output/popute/')) {
      fs.mkdirSync('output/popute/', { recursive: true })
    }

    const imageUrls: string[] = []
    for (const image of images) {
      const buffer = await this.fetchImageBuffer(image, logger)
      if (!buffer) {
        continue
      }

      const trimmedBuffer = await sharp(buffer)
        .trim()
        .extend({
          top: 10,
          bottom: 10,
          left: 10,
          right: 10,
          background: { r: 255, g: 255, b: 255, alpha: 1 },
        })
        .jpeg({ quality: 90 })
        .toBuffer()

      const hash = this.hash(trimmedBuffer)
      fs.writeFileSync(`output/popute/${hash}.jpg`, trimmedBuffer)
      imageUrls.push(`https://book000.github.io/rss-deliver/popute/${hash}.jpg`)
    }
    return imageUrls
  }

  private async fetchImageBuffer(
    image: string,
    logger: Logger
  ): Promise<Buffer | null> {
    if (image.startsWith('data:image/')) {
      const base64 = image.replace(/^data:image\/\w+;base64,/, '')
      return Buffer.from(base64, 'base64')
    }

    const imageUrl = PopTeamEpic.normalizeUrl(image)
    const response = await axios.get(imageUrl, {
      responseType: 'arraybuffer',
      validateStatus: () => true,
    })
    if (response.status !== 200) {
      logger.warn(
        `â— Failed to download image (${response.status}) ${imageUrl}`
      )
      return null
    }
    return Buffer.from(response.data)
  }

  private parseJstDate(dateText: string): Date | null {
    if (!dateText) {
      return null
    }
    const [year, month, day] = dateText.split('/')
    if (!year || !month || !day) {
      return null
    }
    return new Date(`${year}-${month}-${day}T00:00:00+09:00`)
  }

  private static normalizeUrl(url: string): string {
    if (url.startsWith('//')) {
      return `https:${url}`
    }
    if (url.startsWith('/')) {
      return new URL(url, 'https://takecomic.jp').toString()
    }
    return url
  }

  hash(buffer: Buffer): string {
    const hash = crypto.createHash('md5')
    hash.update(buffer)
    return hash.digest('hex')
  }
}
